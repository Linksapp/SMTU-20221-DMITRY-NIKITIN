import numpy as np  # импортируем numpy как np

np.random.seed(3)   # задаём псевдослучайность для numpy (seed из random не влияет на рандом из numpy)
LEARNING_RATE = 0.1  # задаём скорость обучения
index_list = [0, 1, 2, 3]   # записываем все возможные индексы

x_train = [np.array([1.0, -1.0, -1.0]), # задаём список из массивов входных х-ов
           np.array([1.0, -1.0, 1.0]),
           np.array([1.0, 1.0, -1.0]),
           np.array([1.0, 1.0, 1.0])]
y_train = [0.0, 1.0, 1.0, 0.0] # Задаём выходные значения y (сигмоидальная функция равна 1/1+e^(-x))

def neuron_w(input_count):  # определяем функцию, которая возвращает произвольные веса
    weights = np.zeros(input_count+1)   # создаём нулевую матрицу 1 на 3
    for i in range(1, (input_count+1)): # проходимся по значениям этой матрицы, начиная с элемента под индексом 1
        weights[i] = np.random.uniform(-1.0, 1.0)   # изменяем i-е значение weights на случайное в диапазоне [-1, 1)
    return weights  # возвращаем weights

n_w = [neuron_w(2), neuron_w(2), neuron_w(2)]   # создаем список, заполненный массивами, которые заполнены произвольными весами(кроме нулевого веса, он равен 0) 
n_y = [0, 0, 0] # задаём выходные значения для каждого нейрона (список состоящий из 3-ёх нулей)
n_error = [0, 0, 0] # создаём список состоящий из 3-ёх нулей 

def show_learning(): # определяем функцию, которая выводит веса всех нейронов
    print('Current weights:') # выводим текст 'Current weights:' в консоль
    for i, w in enumerate(n_w): # с помощью функции enumerate проходимся по списку n_w, где i - индекс элемента w
        print('neuron ', i, ': w0 =', '%5.2f' % w[0],   # выводим веса i-того нейрона
              ', w1 =', '%5.2f' % w[1], ', w2 =',
              '%5.2f' % w[2])
    print('----------------') # выводим текст '----------------' в консоль

def forward_pass(x): # определяем функцию, которая подсчитывает выходные значения каждого нейрона
    global n_y # делаем переменную n_y глобальной
    n_y[0] = np.tanh(np.dot(n_w[0], x)) # вычисляем гиперболический тангенс (функция активации) скаляра (списка n_w[0] скалярно умноженного на список x)
    n_y[1] = np.tanh(np.dot(n_w[1], x)) # вычисляем гиперболический тангенс (функция активации) скаляра (списка n_w[1] скалярно умноженного на список x)
    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # создаём массив входных значений для второго нейрона
    z2 = np.dot(n_w[2], n2_inputs) # скалярно умножаем n_w[2] на n2_inputs
    n_y[2] = 1.0 / (1.0 + np.exp(-z2)) # сигмоидальная функция

def backward_pass(y_truth): # определяем функцию, вычисляющая функцию активации, ошибки и погрешность
    global n_error # делаем переменную n_error глобальной
    error_prime = -(y_truth - n_y[2])  # вычисляем функцию ошибки
    derivative = n_y[2] * (1.0 - n_y[2])  # вычисляем функцию активации для выходного нейрона
    n_error[2] = error_prime * derivative  # вычисляем погрешность выходного нейрона 
    derivative = 1.0 - n_y[0]**2 # вычисляем функцию активации для выходного нейрона
    n_error[0] = n_w[2][1] * n_error[2] * derivative # вычисляем погрешность выходного нейрона 
    derivative = 1.0 - n_y[1]**2  # вычисляем функцию активации для выходного нейрона
    n_error[1] = n_w[2][2] * n_error[2] * derivative # вычисляем погрешность выходного нейрона 

def adjust_weights(x): # определяем функцию, которая корректирует веса
    global n_w # делаем переменную n_w глобальной
    n_w[0] -= (x * LEARNING_RATE * n_error[0])  # задаём первый вес путём вычитания из массива n_w[0] массива x, умноженного на произведение LEARNING_RATE и n_error[0]
    n_w[1] -= (x * LEARNING_RATE * n_error[1])  # задаём второй вес путём вычитания из массива n_w[1] массива x, умноженного на произведение LEARNING_RATE и n_error[1]
    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # создаём массив входных данных
    n_w[2] -= (n2_inputs * LEARNING_RATE * n_error[2]) # задаем третий вес путём вычитания из массива n_w[2] массива n2_inputs, умноженного на произведение LEARNING_RATE и n_error[2]

# Веса обновляются всегда, тк y cигмоидальной функции - (0, 1)
all_correct = False  # задаём предполагаемую правильность выставленных весов
while not all_correct: # Обучаем сеть поа веса не будут подобраны правильно
    all_correct = True  # если веса подобраны правильно, то цикл обучения закончится
    np.random.shuffle(index_list) # Задаём случайный порядок
    for i in index_list: # Тренеруем сеть на всех примерах
        forward_pass(x_train[i])    # подсчитываем выходные значения всех нейронов
        backward_pass(y_train[i])   # вычисляем погрешность
        adjust_weights(x_train[i])  # корректируем веса
        show_learning() # Выводим обновленные веса
    for i in range(len(x_train)): # Проверяем на сходимость
        forward_pass(x_train[i]) # подсчитываем выходные значения всех нейронов на новых весах
        print('x1 =', '%4.1f' % x_train[i][1], ', x2 =',    # выводим x1, x2 и y
              '%4.1f' % x_train[i][2], ', y =',
              '%.4f' % n_y[2])
        if(((y_train[i] < 0.5) and (n_y[2] >= 0.5))  # проверяем на несходимость
                or ((y_train[i] >= 0.5) and (n_y[2] < 0.5))):
            all_correct = False # не заканчиваем цикл обучения
